
<img width="800" alt="Machine Learning Overview" src="https://github.com/StefaneeT/RA-Statistics-Course/assets/89051155/def3c846-e3de-4424-8c89-ef967abc9ad5">


### Support Vector Machines (SVM): Supervised learning algorithm used for classification and regression analysis.
Constructs a hyperplane in a high-dimensional space to separate classes.


Discriminant Analysis:

A statistical technique used for classification and dimensionality reduction.
It seeks to find the linear combination of features that characterizes or separates two or more classes.
Naive Bayes:

A probabilistic classifier based on Bayes' theorem.
Assumes that the presence of a particular feature in a class is independent of the presence of other features.
Nearest Neighbor:

A simple algorithm that stores all available cases and classifies new cases based on a similarity measure.
The new case is assigned to the class most common among its k nearest neighbors.
Neural Networks:

Computational models inspired by the structure and function of the human brain.
Composed of interconnected nodes (neurons) that process information using activation functions.
Decision Trees:

A decision support tool that uses a tree-like graph of decisions and their possible consequences.
Each internal node represents a "test" on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label.
Ensemble Models:

Techniques that combine multiple individual models to improve predictive performance.
Examples include Random Forest, Gradient Boosting Machines (GBM), and AdaBoost.
Non-linear Regression:

Regression analysis where the relationship between the independent variables and the dependent variable is modeled as non-linear.
Uses methods like polynomial regression, exponential regression, or spline regression.
Linear Regression:

A linear approach to modeling the relationship between a dependent variable and one or more independent variables.
It assumes a linear relationship between the independent and dependent variables.
K-means:

Unsupervised learning algorithm used for clustering.
Divides a set of samples into clusters of approximately equal variance, minimizing a criterion known as the inertia or within-cluster sum-of-squares.
Hierarchical Clustering:

An unsupervised clustering algorithm that builds a hierarchy of clusters.
Can be agglomerative (bottom-up) or divisive (top-down).
Gaussian Mixture Models:

A probabilistic model used for representing the presence of subpopulations within an overall population.
Assumes that all data points are generated from a mixture of several Gaussian distributions.
Hidden Markov Models (HMM):

Statistical models used to model sequences of observable events (e.g., speech, handwriting).
Consists of states, transitions between states, and emission probabilities associated with each state.
